import asyncio
from datetime import datetime
from src.captcha.captchaSolver import solve_audio_captcha
from src.config.browserConfig import setup_browser, cleanup_browser
from src.database.database import get_source_db, get_destination_db
from src.scrapers.leboncoin.utils.human_behavorScrapperLbc import (
    human_like_click_search,
    human_like_delay_search,
    human_like_scroll_to_element_search
)
from playwright.async_api import Page
from loguru import logger

async def scrape_agence_details(page: Page, store_id: str, lien: str) -> dict:
    """Scrape les d√©tails d'une agence √† partir de sa page."""
    update_data = {"scraped": True, "scraped_at": datetime.utcnow()}

    # CodeSiren
    code_siren = "Non trouv√©"
    try:
        siren_locator = page.locator('h3[data-qa-id="siren_number"]')
        if await siren_locator.is_visible(timeout=5000):
            await human_like_scroll_to_element_search(page, siren_locator, scroll_steps=4, jitter=True)
            siren_text = await siren_locator.text_content()
            code_siren = siren_text.replace("SIREN : ", "").strip() if siren_text else "Non trouv√©"
            logger.info(f"‚úÖ CodeSiren trouv√© pour l‚Äôagence {store_id} : {code_siren}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è CodeSiren non trouv√© pour l‚Äôagence {store_id} : {e}")
    update_data["CodeSiren"] = code_siren

    # Adresse
    await human_like_delay_search(0.5, 1.0)
    adresse = "Non trouv√©"
    try:
        adresse_elements = await page.locator("div.flex.gap-sm p").all()
        if adresse_elements:
            text_contents = [await el.text_content() for el in adresse_elements if await el.is_visible(timeout=5000)]
            adresse = " ".join(text.strip() for text in text_contents if text.strip()) or "Non trouv√©"
            logger.info(f"‚úÖ Adresse trouv√©e pour l‚Äôagence {store_id} : {adresse}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Adresse non trouv√©e pour l‚Äôagence {store_id} : {e}")
    update_data["adresse"] = adresse

    # Zone d‚Äôintervention
    await human_like_delay_search(0.5, 1.0)
    zone_intervention = "Non trouv√©"
    try:
        zone_intervention_locator = page.locator("section:has-text('Ce professionnel intervient dans la zone suivante') p")
        if await zone_intervention_locator.is_visible(timeout=5000):
            await human_like_scroll_to_element_search(page, zone_intervention_locator, scroll_steps=4, jitter=True)
            zone_intervention = await zone_intervention_locator.text_content() or "Non trouv√©"
            logger.info(f"‚úÖ Zone d‚Äôintervention trouv√©e pour l‚Äôagence {store_id} : {zone_intervention}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Zone d‚Äôintervention non trouv√©e pour l‚Äôagence {store_id} : {e}")
    update_data["zone_intervention"] = zone_intervention

    # Site Web
    await human_like_delay_search(0.5, 1.0)
    site_web = "Non trouv√©"
    try:
        site_web_locator = page.locator("a[data-qa-id='link_pro_website']")
        if await site_web_locator.is_visible(timeout=5000):
            await human_like_scroll_to_element_search(page, site_web_locator, scroll_steps=4, jitter=True)
            site_web = await site_web_locator.get_attribute("href") or "Non trouv√©"
            logger.info(f"‚úÖ Site web trouv√© pour l‚Äôagence {store_id} : {site_web}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Site web non trouv√© pour l‚Äôagence {store_id} : {e}")
    update_data["siteWeb"] = site_web

    # Horaires
    await human_like_delay_search(0.5, 1.0)
    horaires = "Non trouv√©"
    try:
        horaires_locator = page.locator("div[data-qa-id='company_timesheet']")
        if await horaires_locator.is_visible(timeout=5000):
            await human_like_scroll_to_element_search(page, horaires_locator, scroll_steps=4, jitter=True)
            horaires = await horaires_locator.text_content() or "Non trouv√©"
            logger.info(f"‚úÖ Horaires trouv√©s pour l‚Äôagence {store_id} : {horaires}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Horaires non trouv√©s pour l‚Äôagence {store_id} : {e}")
    update_data["horaires"] = horaires

    # Num√©ro de t√©l√©phone
    await human_like_delay_search(0.5, 1.0)
    numero = "Non trouv√©"
    try:
        phone_tab_button = page.locator("button[role='tab'][id*='radix-'][id$='-trigger-phoneTab']")
        if await phone_tab_button.is_visible(timeout=5000):
            await human_like_scroll_to_element_search(page, phone_tab_button, scroll_steps=4, jitter=True)
            await human_like_click_search(page, phone_tab_button, move_cursor=True, click_delay=0.5)
            await human_like_delay_search(0.5, 2.0)

            display_number_button = page.locator("button[data-qa-id='button_display_phone']")
            if await display_number_button.is_visible(timeout=15000):
                await human_like_scroll_to_element_search(page, display_number_button, scroll_steps=4, jitter=True)
                await human_like_click_search(page, display_number_button, move_cursor=True, click_delay=0.5)
                await human_like_delay_search(0.5, 2.0)

                numero_locator = page.locator("div[data-qa-id='company_phone']")
                if await numero_locator.is_visible(timeout=15000):
                    await human_like_scroll_to_element_search(page, numero_locator, scroll_steps=4, jitter=True)
                    numero = await numero_locator.text_content() or "Non trouv√©"
                    logger.info(f"‚úÖ Num√©ro de t√©l√©phone trouv√© pour l‚Äôagence {store_id} : {numero}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Impossible de scraper le num√©ro pour l‚Äôagence {store_id} : {e}")
    update_data["number"] = numero

    # Description
    await human_like_delay_search(0.5, 1.0)
    description = "Non trouv√©"
    try:
        voir_plus_button = page.locator("div[data-qa-id='company_description'] button:has-text('Voir plus')")
        if await voir_plus_button.is_visible(timeout=2000):
            await human_like_click_search(page, voir_plus_button, move_cursor=True, click_delay=0.5)
            await human_like_delay_search(0.5, 1.5)

        description_locator = page.locator("div[data-qa-id='company_description'] p")
        if await description_locator.is_visible(timeout=10000):
            description = await description_locator.text_content() or "Non trouv√©"
            logger.info(f"‚úÖ Description trouv√©e pour l‚Äôagence {store_id} : {description[:50]}...")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Impossible de scraper la description pour l‚Äôagence {store_id} : {e}")
    update_data["description"] = description

    return update_data

async def scrape_agences(queue):
    """Scrape les agences de la collection agencesBrute qui ne sont pas dans agencesFinale."""
    logger.info("üöÄ D√©marrage du scraping des agences dans agencesBrute...")
    source_db = get_source_db()
    dest_db = get_destination_db()
    agences_brute_collection = source_db["agencesBrute"]
    agences_finale_collection = dest_db["agencesFinale"]

    while True:
        current_hour = datetime.now().hour
        logger.info(f"‚è∞ V√©rification horaire - Heure actuelle : {current_hour}h")
        
        # Ex√©cuter uniquement entre 22h et 10h
        if not (current_hour < 10 or current_hour >= 22):
            logger.info("‚èπÔ∏è Arr√™t temporaire du scraper (horaire diurne). Reprise √† 22h.")
            await asyncio.sleep(3600)  # Attendre 1 heure avant de v√©rifier √† nouveau
            continue

        finale_ids = await agences_finale_collection.distinct("storeId")  # Utiliser storeId ici
        agences = await agences_brute_collection.find({
            "scraped": {"$ne": True},
            "$or": [
                {"storeId": {"$nin": finale_ids}}
            ]
        }).to_list(length=None)
        total_agences = len(agences)
        logger.info(f"üìä Nombre total d'agences √† scraper : {total_agences}")

        if total_agences == 0:
            logger.info("‚ÑπÔ∏è Aucune agence √† scraper dans agencesBrute ou toutes sont d√©j√† dans agencesFinale.")
            await queue.put({"status": "success", "data": {"updated": [], "total": 0, "remaining": 0}})
            await asyncio.sleep(3600)  # Attendre 1 heure avant de recommencer
            continue

        updated_agences = []
        remaining_agences = total_agences
        browser = context = client = profile_id = playwright = None

        try:
            browser, context, client, profile_id, playwright = await setup_browser()
            page = await context.new_page()
            await page.goto("https://www.leboncoin.fr/", timeout=60000)
            await human_like_delay_search(1, 3)

            if await page.locator('iframe[title="DataDome CAPTCHA"]').is_visible(timeout=5000):
                if not await solve_audio_captcha(page):
                    logger.error("‚ùå √âchec de la r√©solution du CAPTCHA initial.")
                    raise Exception("CAPTCHA failure")
                await human_like_delay_search(2, 5)

            cookie_button = page.locator("button", has_text="Accepter")
            if await cookie_button.is_visible(timeout=5000):
                await human_like_click_search(page, cookie_button, move_cursor=True, click_delay=0.2)
                await human_like_delay_search(0.2, 0.5)

            for index, agence in enumerate(agences, 1):
                current_hour = datetime.now().hour
                if current_hour >= 10 and current_hour < 22:
                    logger.info("‚èπÔ∏è Arr√™t forc√© √† 10h du matin. Fermeture du navigateur.")
                    await cleanup_browser(client, profile_id, playwright, browser)
                    browser = context = client = profile_id = playwright = None
                    break

                store_id = agence.get("storeId")
                original_id = agence.get("_id")
                if not store_id:
                    logger.error(f"‚ùå Aucune cl√© 'storeId' trouv√©e pour l'agence {original_id}")
                    remaining_agences -= 1
                    continue

                lien = agence.get("lien")
                logger.info(f"üîç Scraping de l‚Äôagence {store_id} ({index}/{total_agences}) : {lien}")

                agence_page = await context.new_page()
                try:
                    await agence_page.goto(lien, timeout=90000)
                    await human_like_delay_search(1, 3)

                    if await agence_page.locator('iframe[title="DataDome CAPTCHA"]').is_visible(timeout=5000):
                        if not await solve_audio_captcha(agence_page):
                            logger.error(f"‚ùå √âchec de la r√©solution du CAPTCHA pour l‚Äôagence {store_id}.")
                            raise Exception("CAPTCHA failure")

                    update_data = await scrape_agence_details(agence_page, store_id, lien)
                    # Mettre √† jour agencesBrute
                    await agences_brute_collection.update_one(
                        {"_id": original_id},
                        {"$set": update_data}
                    )
                    # Transf√©rer ou mettre √† jour dans agencesFinale avec l'_id original
                    agence_data = await agences_brute_collection.find_one({"_id": original_id})
                    agence_data["_id"] = original_id  # Conserver l'_id original
                    await agences_finale_collection.delete_one({"storeId": store_id})  # Supprimer si existant
                    await agences_finale_collection.insert_one(agence_data)
                    updated_agences.append({"storeId": store_id, "name": agence.get("name"), "_id": str(original_id), **update_data})
                    logger.info(f"‚úÖ Agence {store_id} scrap√©e et transf√©r√©e avec _id: {original_id}")
                except Exception as e:
                    if "CAPTCHA failure" not in str(e):
                        logger.error(f"‚ö†Ô∏è Erreur lors du scraping de l‚Äôagence {store_id} : {e}")
                finally:
                    await agence_page.close()
                remaining_agences -= 1

            if browser:  # Si le navigateur est encore ouvert apr√®s la boucle
                logger.info(f"üèÅ Scraping termin√© - Total agences trait√©es : {total_agences}, mises √† jour : {len(updated_agences)}")
                await queue.put({"status": "success", "data": {"updated": updated_agences, "total": total_agences, "remaining": remaining_agences}})
                await cleanup_browser(client, profile_id, playwright, browser)

        except Exception as e:
            logger.error(f"‚ö†Ô∏è Erreur dans la session : {e}")
            if browser:
                await cleanup_browser(client, profile_id, playwright, browser)
            await asyncio.sleep(10)  # Attendre avant de relancer
            continue