import logging
import random
import asyncio
from multiprocessing import Process, Queue
from src.captcha.captchaSolver import solve_audio_captcha
from src.config.browserConfig import setup_browser, cleanup_browser
from src.scrapers.leboncoin.searchParser import close_cookies_popup, wait_for_page_load, apply_filters, navigate_to_locations, close_gimii_popup
from src.scrapers.leboncoin.listings_parser import scrape_listings_via_api
from playwright.async_api import Page
from src.scrapers.leboncoin.utils.human_behavorScrapperLbc import human_like_exploration, simulate_reading
from loguru import logger

logger = logging.getLogger(__name__)
TARGET_API_URL = "https://api.leboncoin.fr/finder/search"

async def check_and_solve_captcha(page: Page, action: str) -> bool:
    """V√©rifie et r√©sout le CAPTCHA si pr√©sent avant une action."""
    captcha_selector = 'iframe[title="DataDome CAPTCHA"]'
    if await page.locator(captcha_selector).is_visible(timeout=3000):
        logger.warning(f"‚ö†Ô∏è CAPTCHA d√©tect√© avant {action}.")
        if not await solve_audio_captcha(page):
            logger.error(f"‚ùå √âchec de la r√©solution du CAPTCHA avant {action}.")
            return False
        logger.info(f"‚úÖ CAPTCHA r√©solu avant {action}.")
    return True

def run_open_leboncoin(queue):
    asyncio.run(open_leboncoin(queue))

async def open_leboncoin(queue: Queue):
    logger.info("üöÄ D√©marrage du scraping...")
    max_retries = 1
    browser = context = client = profile_id = playwright = None

    try:
        browser, context, client, profile_id, playwright = await setup_browser()
        if not browser or not context:
            logger.error("‚ö†Ô∏è ERREUR: Impossible d'ouvrir le navigateur.")
            queue.put({"status": "error", "message": "Impossible d'ouvrir le navigateur"})
            return
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Erreur lors de l'initialisation du navigateur : {e}")
        queue.put({"status": "error", "message": "√âchec de l'initialisation du navigateur"})
        return

    page = None
    try:
        page = await context.new_page()
        api_responses = []

        async def process_response(response):
            if response.url.startswith(TARGET_API_URL) and response.status == 200:
                try:
                    json_response = await response.json()
                    api_responses.append(json_response)
                    if "ads" in json_response and json_response["ads"]:
                        logger.info(f"üì° {len(json_response['ads'])} annonces : {response.url}")
                    else:
                        logger.debug(f"üì° Sans annonces : {response.url}")
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è Erreur dans la r√©ponse {response.url} : {e}")

        def on_response(response):
            asyncio.create_task(process_response(response))

        page.on("response", on_response)
        logger.info("üåç Acc√®s √† Leboncoin...")
        await page.goto("https://www.leboncoin.fr/", timeout=90000)

        await human_like_exploration(page)
        await simulate_reading(page)
        await asyncio.sleep(random.uniform(1, 3))

        if not await check_and_solve_captcha(page, "premi√®res interactions"):
            queue.put({"status": "error", "message": "√âchec de la r√©solution du CAPTCHA initial"})
            return

        await wait_for_page_load(page)
        await close_cookies_popup(page)

        gimii_closed = await close_gimii_popup(page)
        if gimii_closed:
            logger.info("üîÑ Popup Gimii d√©tect√©e et ferm√©e, nouvelle tentative de chargement.")
            await wait_for_page_load(page)

        if await page.locator("#didomi-popup > div > div > div > span").is_visible(timeout=2000):
            logger.error("‚ö†Ô∏è La pop-up des cookies n‚Äôa pas √©t√© ferm√©e correctement.")
            queue.put({"status": "error", "message": "√âchec fermeture cookies"})
            return

        logger.info("üîç Seconde v√©rification de la popup Gimii avant navigation...")
        gimii_closed_again = await close_gimii_popup(page)
        if gimii_closed_again:
            logger.info("üîÑ Popup Gimii d√©tect√©e une seconde fois et ferm√©e.")
            await wait_for_page_load(page)

        if not await check_and_solve_captcha(page, "navigation vers Locations"):
            queue.put({"status": "error", "message": "√âchec de la r√©solution du CAPTCHA avant navigation Locations"})
            return

        await navigate_to_locations(page)
        initial_response, response_handler = await apply_filters(page, api_responses)

        if not await check_and_solve_captcha(page, "scraping des listings"):
            queue.put({"status": "error", "message": "√âchec de la r√©solution du CAPTCHA avant scraping"})
            return

        # Scraper 100 pages avant de fermer le navigateur
        await scrape_listings_via_api(page, api_responses, response_handler, initial_response)
        title = await page.title()
        logger.info(f"‚úÖ Page ouverte - Titre : {title}")
        queue.put({"status": "success", "title": title})

    except Exception as e:
        logger.error(f"‚ö†Ô∏è Erreur lors du scraping : {e}")
        queue.put({"status": "error", "message": str(e)})
    finally:
        if page:
            try:
                page.remove_listener("response", on_response)
                logger.info("üéôÔ∏è √âcouteur API principal supprim√©.")
            except Exception as e:
                logger.error(f"‚ùå Erreur lors de la suppression de l'√©couteur principal : {e}")
            # Ne pas fermer la page ici, elle reste ouverte jusqu'√† la fin du scraping
        # Ne pas appeler cleanup_browser ici, on le fera apr√®s le scraping des 100 pages

    # Attendre que le scraping soit termin√© (g√©r√© dans scrape_listings_via_api)
    # Le navigateur sera ferm√© dans access_leboncoin apr√®s 100 pages

    await cleanup_browser(client, profile_id, playwright, browser)

async def access_leboncoin():
    queue = Queue()
    process = Process(target=run_open_leboncoin, args=(queue,))
    process.start()
    process.join()
    result = queue.get()

    # Si le scraping est termin√© (100 pages scrapp√©es ou erreur), le navigateur est ferm√© dans open_leboncoin
    if result.get("status") == "success":
        logger.info("‚úÖ Scraping des 100 pages termin√© avec succ√®s.")
    else:
        logger.error("‚ùå Scraping √©chou√© : le navigateur a √©t√© ferm√©, mais tu peux v√©rifier les logs pour plus de d√©tails.")

    return {"status": "success", "message": "Scraping termin√©", "data": result}

if __name__ == "__main__":
    asyncio.run(access_leboncoin())